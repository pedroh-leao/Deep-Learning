{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network To Predict Stock Price\n",
    "\n",
    "Using multiple attributes of the dataset to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.718563</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.970000</td>\n",
       "      <td>15.938125</td>\n",
       "      <td>22173100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>16.139999</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.049999</td>\n",
       "      <td>16.017963</td>\n",
       "      <td>23552200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.129999</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>19011500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1242 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       "1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       "1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       "1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       "1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       "\n",
       "          Volume  \n",
       "0     30182600.0  \n",
       "1     30552600.0  \n",
       "2     36141000.0  \n",
       "3     28069600.0  \n",
       "4     29091300.0  \n",
       "...          ...  \n",
       "1240         0.0  \n",
       "1241  22173100.0  \n",
       "1242  23552200.0  \n",
       "1243  19011500.0  \n",
       "1244         0.0  \n",
       "\n",
       "[1242 rows x 7 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('petr4_training.csv')\n",
    "dataset = dataset.dropna()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9990000e+01, 2.0209999e+01, 1.9690001e+01, 1.9690001e+01,\n",
       "        1.8086271e+01, 3.0182600e+07],\n",
       "       [1.9809999e+01, 2.0400000e+01, 1.9700001e+01, 2.0400000e+01,\n",
       "        1.8738441e+01, 3.0552600e+07],\n",
       "       [2.0330000e+01, 2.0620001e+01, 2.0170000e+01, 2.0430000e+01,\n",
       "        1.8766001e+01, 3.6141000e+07],\n",
       "       ...,\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = dataset.iloc[:, 1:7].values\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "        0.04318274],\n",
       "       [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "        0.0437121 ],\n",
       "       [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "        0.05170752],\n",
       "       ...,\n",
       "       [0.57122093, 0.57537562, 0.60696008, 0.58202356, 0.58202349,\n",
       "        0.03369652],\n",
       "       [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "        0.02720006],\n",
       "       [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = MinMaxScaler(feature_range= (0, 1))\n",
    "dataset_train = normalizer.fit_transform(dataset_train)\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar as variaveis x e y baseado nos 90 preços anteriores\n",
    "x, y = [], []\n",
    "\n",
    "# using the last 90 prices to predict the next\n",
    "for i in range(90, 1242): \n",
    "    x.append(dataset_train[i-90:i, 0:6])\n",
    "    y.append(dataset_train[i, 0])\n",
    "\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "         0.04318274],\n",
       "        [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "         0.0437121 ],\n",
       "        [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "         0.05170752],\n",
       "        [0.78875969, 0.7949588 , 0.81013311, 0.77996075, 0.70144373,\n",
       "         0.04015963],\n",
       "        [0.77083338, 0.77363063, 0.78505624, 0.75147351, 0.67522435,\n",
       "         0.0416214 ],\n",
       "        [0.74806197, 0.75618037, 0.78505624, 0.76031438, 0.68336137,\n",
       "         0.03485382],\n",
       "        [0.75436047, 0.76490543, 0.78915051, 0.76768177, 0.69014234,\n",
       "         0.02507502],\n",
       "        [0.75823643, 0.76442079, 0.79733884, 0.77013751, 0.6924025 ,\n",
       "         0.0260728 ],\n",
       "        [0.76598837, 0.77411537, 0.79682707, 0.76227897, 0.68516964,\n",
       "         0.0404927 ],\n",
       "        [0.76598837, 0.77411537, 0.79682707, 0.76719061, 0.68969016,\n",
       "         0.0423977 ],\n",
       "        [0.76017437, 0.75714973, 0.79222108, 0.76817293, 0.69059437,\n",
       "         0.02401858],\n",
       "        [0.75872098, 0.75908871, 0.79222108, 0.76178781, 0.68471746,\n",
       "         0.02821315],\n",
       "        [0.75581391, 0.75714973, 0.78915051, 0.75540279, 0.6788408 ,\n",
       "         0.02706042],\n",
       "        [0.74467054, 0.74309258, 0.77533265, 0.74607071, 0.67025175,\n",
       "         0.02587622],\n",
       "        [0.7374031 , 0.74357736, 0.77328557, 0.75540279, 0.6788408 ,\n",
       "         0.03367205],\n",
       "        [0.7374031 , 0.74454673, 0.77328557, 0.75392926, 0.67748471,\n",
       "         0.02460946],\n",
       "        [0.73498067, 0.75036355, 0.78045041, 0.75687631, 0.68019705,\n",
       "         0.02806007],\n",
       "        [0.75242248, 0.75327189, 0.77533265, 0.74508849, 0.66934774,\n",
       "         0.02878973],\n",
       "        [0.73401163, 0.73194382, 0.75332651, 0.73231836, 0.65759427,\n",
       "         0.03876941],\n",
       "        [0.71656977, 0.71352399, 0.71903787, 0.68762287, 0.6164569 ,\n",
       "         0.09583767],\n",
       "        [0.68120155, 0.68153175, 0.70522006, 0.68172891, 0.61103237,\n",
       "         0.04756616],\n",
       "        [0.67538755, 0.69704314, 0.71647907, 0.70039291, 0.62821037,\n",
       "         0.04129104],\n",
       "        [0.67635659, 0.68250121, 0.70470824, 0.67779964, 0.60741587,\n",
       "         0.04620398],\n",
       "        [0.63372098, 0.67959287, 0.67246673, 0.68172891, 0.61103237,\n",
       "         0.11064144],\n",
       "        [0.66521318, 0.66553563, 0.6862846 , 0.65815327, 0.58933361,\n",
       "         0.04418925],\n",
       "        [0.65649225, 0.66456617, 0.67553736, 0.65324168, 0.584813  ,\n",
       "         0.0530315 ],\n",
       "        [0.64680228, 0.65487159, 0.67860793, 0.6650295 , 0.5956623 ,\n",
       "         0.04444964],\n",
       "        [0.66618222, 0.66553563, 0.69651996, 0.66797641, 0.59837464,\n",
       "         0.03194532],\n",
       "        [0.65843028, 0.66068832, 0.6888434 , 0.66159139, 0.59249793,\n",
       "         0.0370597 ],\n",
       "        [0.64970935, 0.65535633, 0.6862846 , 0.6596267 , 0.59068976,\n",
       "         0.0357702 ],\n",
       "        [0.65116274, 0.66311202, 0.68577277, 0.67288805, 0.60289526,\n",
       "         0.02903152],\n",
       "        [0.66424419, 0.67426079, 0.70470824, 0.68271123, 0.61193639,\n",
       "         0.0412361 ],\n",
       "        [0.67344961, 0.67038294, 0.68730803, 0.65913564, 0.59023768,\n",
       "         0.03711206],\n",
       "        [0.64292631, 0.6446922 , 0.66939616, 0.64440082, 0.57667593,\n",
       "         0.04346845],\n",
       "        [0.64486434, 0.64178381, 0.65967247, 0.63605111, 0.56899095,\n",
       "         0.04421171],\n",
       "        [0.62257747, 0.62190984, 0.65148414, 0.62622798, 0.55994986,\n",
       "         0.04364257],\n",
       "        [0.60949617, 0.61027635, 0.63510752, 0.61591359, 0.55045665,\n",
       "         0.04779322],\n",
       "        [0.60998067, 0.61609307, 0.6407369 , 0.61935165, 0.55362107,\n",
       "         0.04092922],\n",
       "        [0.60852713, 0.60979157, 0.63613096, 0.60952857, 0.54457989,\n",
       "         0.03981569],\n",
       "        [0.59593023, 0.61803199, 0.62845445, 0.62377213, 0.55768961,\n",
       "         0.04509603],\n",
       "        [0.61143411, 0.62190984, 0.63254862, 0.60412577, 0.5396073 ,\n",
       "         0.05085238],\n",
       "        [0.60222863, 0.60542899, 0.6320368 , 0.60707267, 0.54231954,\n",
       "         0.04531064],\n",
       "        [0.64922481, 0.67862336, 0.6704196 , 0.68025539, 0.60967603,\n",
       "         0.10572707],\n",
       "        [0.68362398, 0.74212312, 0.72620261, 0.72445981, 0.65036132,\n",
       "         0.08930445],\n",
       "        [0.70687989, 0.72952012, 0.7185261 , 0.69597258, 0.62414194,\n",
       "         0.04376518],\n",
       "        [0.68265509, 0.71255448, 0.7062436 , 0.72347744, 0.64945705,\n",
       "         0.03589495],\n",
       "        [0.70978682, 0.72079491, 0.74257927, 0.71414542, 0.64086801,\n",
       "         0.03739277],\n",
       "        [0.70784879, 0.72370339, 0.74769703, 0.71463658, 0.64132019,\n",
       "         0.04530406],\n",
       "        [0.71608527, 0.73242845, 0.74104401, 0.74115922, 0.66573124,\n",
       "         0.03887614],\n",
       "        [0.73643411, 0.74066888, 0.76202661, 0.73133599, 0.65669001,\n",
       "         0.06269313],\n",
       "        [0.7122093 , 0.73097431, 0.75332651, 0.73673879, 0.6616627 ,\n",
       "         0.05787405],\n",
       "        [0.7122093 , 0.73097431, 0.75281474, 0.73182715, 0.65714209,\n",
       "         0.04839097],\n",
       "        [0.7194767 , 0.72176442, 0.74513818, 0.71954817, 0.6458407 ,\n",
       "         0.03954013],\n",
       "        [0.70348832, 0.70722254, 0.73541453, 0.70383112, 0.63137489,\n",
       "         0.03144514],\n",
       "        [0.69525189, 0.69995148, 0.73387917, 0.70874262, 0.63589531,\n",
       "         0.02308847],\n",
       "        [0.70397287, 0.70528357, 0.73183214, 0.70677803, 0.63408723,\n",
       "         0.03482392],\n",
       "        [0.70397287, 0.7081919 , 0.73490276, 0.70677803, 0.63408723,\n",
       "         0.02257928],\n",
       "        [0.69767442, 0.69510427, 0.72824974, 0.69842833, 0.6264022 ,\n",
       "         0.01903582],\n",
       "        [0.68168605, 0.68395536, 0.71136131, 0.67927317, 0.60877212,\n",
       "         0.02224034],\n",
       "        [0.68168605, 0.68395536, 0.69344933, 0.66306491, 0.59385423,\n",
       "         0.02942397],\n",
       "        [0.65310078, 0.66650509, 0.69396111, 0.67779964, 0.60741587,\n",
       "         0.02244093],\n",
       "        [0.66618222, 0.67571493, 0.6949847 , 0.66355598, 0.59430621,\n",
       "         0.02782257],\n",
       "        [0.64825581, 0.66117305, 0.68730803, 0.66797641, 0.59837464,\n",
       "         0.02440802],\n",
       "        [0.66182175, 0.66117305, 0.6765609 , 0.64685666, 0.57893629,\n",
       "         0.03144357],\n",
       "        [0.64341085, 0.6776539 , 0.68372569, 0.68516703, 0.61419665,\n",
       "         0.04400526],\n",
       "        [0.67877902, 0.69704314, 0.71903787, 0.69842833, 0.6264022 ,\n",
       "         0.04546845],\n",
       "        [0.69137592, 0.69122642, 0.7036848 , 0.67730848, 0.60696374,\n",
       "         0.03177292],\n",
       "        [0.66569772, 0.66941348, 0.6862846 , 0.67583495, 0.6056075 ,\n",
       "         0.03919891],\n",
       "        [0.65406982, 0.6572952 , 0.665302  , 0.63998039, 0.57260735,\n",
       "         0.05120333],\n",
       "        [0.64292631, 0.65341735, 0.68116684, 0.66306491, 0.59385423,\n",
       "         0.03397579],\n",
       "        [0.64147292, 0.64614639, 0.65813715, 0.63703343, 0.56989516,\n",
       "         0.05635362],\n",
       "        [0.63565891, 0.66262729, 0.665302  , 0.66895878, 0.5992789 ,\n",
       "         0.04077971],\n",
       "        [0.67587209, 0.68880271, 0.70777897, 0.6969548 , 0.625046  ,\n",
       "         0.0548714 ],\n",
       "        [0.68653106, 0.70382942, 0.71903787, 0.71660126, 0.64312846,\n",
       "         0.03461346],\n",
       "        [0.70300383, 0.73921474, 0.74411464, 0.73280952, 0.6580463 ,\n",
       "         0.04969664],\n",
       "        [0.71996119, 0.74600097, 0.76202661, 0.74852661, 0.67251211,\n",
       "         0.04766145],\n",
       "        [0.73982553, 0.74745521, 0.76867958, 0.73526526, 0.66030651,\n",
       "         0.05031056],\n",
       "        [0.76550388, 0.79059622, 0.80962134, 0.79666016, 0.71681365,\n",
       "         0.10120858],\n",
       "        [0.74854651, 0.76732913, 0.7840328 , 0.7804519 , 0.71911682,\n",
       "         0.06567045],\n",
       "        [0.75823643, 0.79301983, 0.80501535, 0.78831045, 0.72648688,\n",
       "         0.04828195],\n",
       "        [0.78924419, 0.79447407, 0.80706238, 0.77455795, 0.71358928,\n",
       "         0.06152981],\n",
       "        [0.76598837, 0.78041692, 0.80348004, 0.79223972, 0.73017189,\n",
       "         0.04455508],\n",
       "        [0.78488372, 0.79835191, 0.82702155, 0.80648339, 0.74353023,\n",
       "         0.03775975],\n",
       "        [0.80184109, 0.80222976, 0.82395082, 0.79027514, 0.72832946,\n",
       "         0.03492235],\n",
       "        [0.77761628, 0.78768783, 0.81729785, 0.7907662 , 0.72879006,\n",
       "         0.03271233],\n",
       "        [0.77325581, 0.78138628, 0.79785051, 0.77406679, 0.71312854,\n",
       "         0.0315204 ],\n",
       "        [0.7562984 , 0.75521086, 0.78096208, 0.75098236, 0.69147899,\n",
       "         0.03087142],\n",
       "        [0.74273261, 0.74697043, 0.77430911, 0.75392926, 0.69424286,\n",
       "         0.04384244],\n",
       "        [0.74127907, 0.74503146, 0.77840328, 0.75491163, 0.6951641 ,\n",
       "         0.03128876],\n",
       "        [0.74224806, 0.76635967, 0.78505624, 0.76375249, 0.7034554 ,\n",
       "         0.03586405]]),\n",
       " (1152, 90, 6),\n",
       " 0.7611434108527131,\n",
       " (1152,))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], x.shape, y[0], y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\Documents\\codigos\\Deep-Learning\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units= 100, return_sequences=True, input_shape= (x.shape[1], 6)))\n",
    "model.add(Dropout(rate= 0.3))\n",
    "\n",
    "model.add(LSTM(units= 50, return_sequences=True))\n",
    "model.add(Dropout(rate= 0.3))\n",
    "\n",
    "model.add(LSTM(units= 50, return_sequences=True))\n",
    "model.add(Dropout(rate= 0.3))\n",
    "\n",
    "model.add(LSTM(units= 50))\n",
    "model.add(Dropout(rate= 0.3))\n",
    "\n",
    "model.add(Dense(units= 1, activation= 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= tf.keras.optimizers.Adam(), loss= 'mean_squared_error', metrics= ['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "es = EarlyStopping(monitor= 'loss', patience= 10, verbose= True, min_delta= 1e-10)\n",
    "rlr = ReduceLROnPlateau(monitor= 'loss', patience= 5, verbose= 1, factor= 0.2)\n",
    "mcp = ModelCheckpoint(filepath= 'weights.keras', monitor= 'loss', save_best_only= True, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0519 - mean_absolute_error: 0.1679\n",
      "Epoch 1: loss improved from inf to 0.02479, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 0.0511 - mean_absolute_error: 0.1664 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0083 - mean_absolute_error: 0.0689\n",
      "Epoch 2: loss improved from 0.02479 to 0.00805, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.0083 - mean_absolute_error: 0.0689 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0075 - mean_absolute_error: 0.0658\n",
      "Epoch 3: loss improved from 0.00805 to 0.00795, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0075 - mean_absolute_error: 0.0659 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0075 - mean_absolute_error: 0.0669\n",
      "Epoch 4: loss improved from 0.00795 to 0.00694, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.0075 - mean_absolute_error: 0.0668 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0070 - mean_absolute_error: 0.0651\n",
      "Epoch 5: loss improved from 0.00694 to 0.00636, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0070 - mean_absolute_error: 0.0650 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0068 - mean_absolute_error: 0.0623\n",
      "Epoch 6: loss did not improve from 0.00636\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.0068 - mean_absolute_error: 0.0624 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0065 - mean_absolute_error: 0.0603\n",
      "Epoch 7: loss improved from 0.00636 to 0.00635, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.0065 - mean_absolute_error: 0.0603 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0059 - mean_absolute_error: 0.0592\n",
      "Epoch 8: loss improved from 0.00635 to 0.00593, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0059 - mean_absolute_error: 0.0592 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0063 - mean_absolute_error: 0.0602\n",
      "Epoch 9: loss improved from 0.00593 to 0.00578, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0063 - mean_absolute_error: 0.0601 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0058 - mean_absolute_error: 0.0569\n",
      "Epoch 10: loss improved from 0.00578 to 0.00531, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0058 - mean_absolute_error: 0.0569 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0048 - mean_absolute_error: 0.0528\n",
      "Epoch 11: loss improved from 0.00531 to 0.00499, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0048 - mean_absolute_error: 0.0528 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0048 - mean_absolute_error: 0.0535\n",
      "Epoch 12: loss improved from 0.00499 to 0.00497, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0048 - mean_absolute_error: 0.0535 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0049 - mean_absolute_error: 0.0537\n",
      "Epoch 13: loss did not improve from 0.00497\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0049 - mean_absolute_error: 0.0537 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0046 - mean_absolute_error: 0.0507\n",
      "Epoch 14: loss improved from 0.00497 to 0.00466, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0046 - mean_absolute_error: 0.0508 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0047 - mean_absolute_error: 0.0519\n",
      "Epoch 15: loss improved from 0.00466 to 0.00444, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0047 - mean_absolute_error: 0.0519 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0044 - mean_absolute_error: 0.0513\n",
      "Epoch 16: loss did not improve from 0.00444\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0044 - mean_absolute_error: 0.0513 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0047 - mean_absolute_error: 0.0503\n",
      "Epoch 17: loss did not improve from 0.00444\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0047 - mean_absolute_error: 0.0503 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0044 - mean_absolute_error: 0.0510\n",
      "Epoch 18: loss improved from 0.00444 to 0.00419, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0044 - mean_absolute_error: 0.0509 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0041 - mean_absolute_error: 0.0491\n",
      "Epoch 19: loss improved from 0.00419 to 0.00384, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.0041 - mean_absolute_error: 0.0491 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0038 - mean_absolute_error: 0.0474\n",
      "Epoch 20: loss did not improve from 0.00384\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0038 - mean_absolute_error: 0.0474 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0039 - mean_absolute_error: 0.0462\n",
      "Epoch 21: loss did not improve from 0.00384\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0039 - mean_absolute_error: 0.0462 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0039 - mean_absolute_error: 0.0463\n",
      "Epoch 22: loss did not improve from 0.00384\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0039 - mean_absolute_error: 0.0463 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0037 - mean_absolute_error: 0.0462\n",
      "Epoch 23: loss improved from 0.00384 to 0.00332, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0037 - mean_absolute_error: 0.0461 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0032 - mean_absolute_error: 0.0429\n",
      "Epoch 24: loss did not improve from 0.00332\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0032 - mean_absolute_error: 0.0430 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0041 - mean_absolute_error: 0.0485\n",
      "Epoch 25: loss did not improve from 0.00332\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.0041 - mean_absolute_error: 0.0484 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0037 - mean_absolute_error: 0.0457\n",
      "Epoch 26: loss did not improve from 0.00332\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.0037 - mean_absolute_error: 0.0457 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0031 - mean_absolute_error: 0.0421\n",
      "Epoch 27: loss improved from 0.00332 to 0.00324, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.0031 - mean_absolute_error: 0.0421 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0033 - mean_absolute_error: 0.0440\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 28: loss did not improve from 0.00324\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0033 - mean_absolute_error: 0.0440 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0037 - mean_absolute_error: 0.0471\n",
      "Epoch 29: loss improved from 0.00324 to 0.00320, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0037 - mean_absolute_error: 0.0470 - learning_rate: 2.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0031 - mean_absolute_error: 0.0425\n",
      "Epoch 30: loss improved from 0.00320 to 0.00306, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0031 - mean_absolute_error: 0.0425 - learning_rate: 2.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0027 - mean_absolute_error: 0.0395\n",
      "Epoch 31: loss improved from 0.00306 to 0.00284, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0027 - mean_absolute_error: 0.0395 - learning_rate: 2.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0028 - mean_absolute_error: 0.0393\n",
      "Epoch 32: loss improved from 0.00284 to 0.00281, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0028 - mean_absolute_error: 0.0393 - learning_rate: 2.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0027 - mean_absolute_error: 0.0385\n",
      "Epoch 33: loss improved from 0.00281 to 0.00276, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0027 - mean_absolute_error: 0.0385 - learning_rate: 2.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0025 - mean_absolute_error: 0.0378\n",
      "Epoch 34: loss improved from 0.00276 to 0.00270, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0025 - mean_absolute_error: 0.0378 - learning_rate: 2.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0027 - mean_absolute_error: 0.0391\n",
      "Epoch 35: loss did not improve from 0.00270\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0027 - mean_absolute_error: 0.0391 - learning_rate: 2.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0028 - mean_absolute_error: 0.0402\n",
      "Epoch 36: loss improved from 0.00270 to 0.00264, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0028 - mean_absolute_error: 0.0402 - learning_rate: 2.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0029 - mean_absolute_error: 0.0410\n",
      "Epoch 37: loss did not improve from 0.00264\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0029 - mean_absolute_error: 0.0410 - learning_rate: 2.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0027 - mean_absolute_error: 0.0390\n",
      "Epoch 38: loss did not improve from 0.00264\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0027 - mean_absolute_error: 0.0390 - learning_rate: 2.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0024 - mean_absolute_error: 0.0373\n",
      "Epoch 39: loss improved from 0.00264 to 0.00259, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0024 - mean_absolute_error: 0.0373 - learning_rate: 2.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0027 - mean_absolute_error: 0.0390\n",
      "Epoch 40: loss did not improve from 0.00259\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0027 - mean_absolute_error: 0.0390 - learning_rate: 2.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0027 - mean_absolute_error: 0.0396\n",
      "Epoch 41: loss did not improve from 0.00259\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0027 - mean_absolute_error: 0.0396 - learning_rate: 2.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m35/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0031 - mean_absolute_error: 0.0414\n",
      "Epoch 42: loss did not improve from 0.00259\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 0.0031 - mean_absolute_error: 0.0413 - learning_rate: 2.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0026 - mean_absolute_error: 0.0376\n",
      "Epoch 43: loss improved from 0.00259 to 0.00259, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0026 - mean_absolute_error: 0.0377 - learning_rate: 2.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0027 - mean_absolute_error: 0.0402\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 44: loss did not improve from 0.00259\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0027 - mean_absolute_error: 0.0402 - learning_rate: 2.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0024 - mean_absolute_error: 0.0372\n",
      "Epoch 45: loss improved from 0.00259 to 0.00245, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0024 - mean_absolute_error: 0.0372 - learning_rate: 4.0000e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0024 - mean_absolute_error: 0.0381\n",
      "Epoch 46: loss did not improve from 0.00245\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0024 - mean_absolute_error: 0.0381 - learning_rate: 4.0000e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0024 - mean_absolute_error: 0.0373\n",
      "Epoch 47: loss improved from 0.00245 to 0.00239, saving model to weights.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0024 - mean_absolute_error: 0.0373 - learning_rate: 4.0000e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0026 - mean_absolute_error: 0.0385\n",
      "Epoch 48: loss did not improve from 0.00239\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0026 - mean_absolute_error: 0.0384 - learning_rate: 4.0000e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0024 - mean_absolute_error: 0.0367\n",
      "Epoch 49: loss did not improve from 0.00239\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.0024 - mean_absolute_error: 0.0367 - learning_rate: 4.0000e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0024 - mean_absolute_error: 0.0374\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 50: loss did not improve from 0.00239\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0024 - mean_absolute_error: 0.0374 - learning_rate: 4.0000e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0029 - mean_absolute_error: 0.0399\n",
      "Epoch 51: loss did not improve from 0.00239\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0029 - mean_absolute_error: 0.0399 - learning_rate: 8.0000e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0025 - mean_absolute_error: 0.0379\n",
      "Epoch 52: loss did not improve from 0.00239\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.0025 - mean_absolute_error: 0.0379 - learning_rate: 8.0000e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0024 - mean_absolute_error: 0.0381\n",
      "Epoch 53: loss did not improve from 0.00239\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0024 - mean_absolute_error: 0.0381 - learning_rate: 8.0000e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0026 - mean_absolute_error: 0.0382\n",
      "Epoch 54: loss did not improve from 0.00239\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.0026 - mean_absolute_error: 0.0382 - learning_rate: 8.0000e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0028 - mean_absolute_error: 0.0386\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 55: loss did not improve from 0.00239\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.0028 - mean_absolute_error: 0.0386 - learning_rate: 8.0000e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0027 - mean_absolute_error: 0.0385\n",
      "Epoch 56: loss did not improve from 0.00239\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0027 - mean_absolute_error: 0.0385 - learning_rate: 1.6000e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0028 - mean_absolute_error: 0.0404\n",
      "Epoch 57: loss did not improve from 0.00239\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.0028 - mean_absolute_error: 0.0403 - learning_rate: 1.6000e-06\n",
      "Epoch 57: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ca282fdd90>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    epochs= 100,\n",
    "    batch_size= 32,\n",
    "    callbacks= [es, rlr, mcp]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.516966</td>\n",
       "      <td>33461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>16.490000</td>\n",
       "      <td>16.719999</td>\n",
       "      <td>16.370001</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.666668</td>\n",
       "      <td>55940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>16.780001</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>16.620001</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>16.696608</td>\n",
       "      <td>37064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>16.570000</td>\n",
       "      <td>16.830000</td>\n",
       "      <td>16.796408</td>\n",
       "      <td>26958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>16.740000</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.709999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>28400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>35070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>17.049999</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>16.799999</td>\n",
       "      <td>16.766466</td>\n",
       "      <td>28547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>16.879999</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>16.840000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>17.215569</td>\n",
       "      <td>37921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>17.040001</td>\n",
       "      <td>17.410000</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.265469</td>\n",
       "      <td>45912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>17.315371</td>\n",
       "      <td>28945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>17.840000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>17.614771</td>\n",
       "      <td>58618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>17.920000</td>\n",
       "      <td>18.360001</td>\n",
       "      <td>17.809999</td>\n",
       "      <td>18.360001</td>\n",
       "      <td>18.323355</td>\n",
       "      <td>58488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>18.530001</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>18.219999</td>\n",
       "      <td>18.183632</td>\n",
       "      <td>48575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>18.309999</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>18.030001</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.223553</td>\n",
       "      <td>33470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.469999</td>\n",
       "      <td>18.090000</td>\n",
       "      <td>18.469999</td>\n",
       "      <td>18.433134</td>\n",
       "      <td>33920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>18.459999</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.240000</td>\n",
       "      <td>18.203592</td>\n",
       "      <td>35567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>19.629999</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>89768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966   \n",
       "1   2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668   \n",
       "2   2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608   \n",
       "3   2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408   \n",
       "4   2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010   \n",
       "5   2018-01-09  17.030001  17.160000  16.959999  17.030001  16.996010   \n",
       "6   2018-01-10  16.920000  17.049999  16.770000  16.799999  16.766466   \n",
       "7   2018-01-11  16.879999  17.299999  16.840000  17.250000  17.215569   \n",
       "8   2018-01-12  17.040001  17.410000  17.020000  17.299999  17.265469   \n",
       "9   2018-01-15  17.320000  17.440001  17.150000  17.350000  17.315371   \n",
       "10  2018-01-16  17.350000  17.840000  17.299999  17.650000  17.614771   \n",
       "11  2018-01-17  17.920000  18.360001  17.809999  18.360001  18.323355   \n",
       "12  2018-01-18  18.350000  18.530001  17.930000  18.219999  18.183632   \n",
       "13  2018-01-19  18.309999  18.420000  18.030001  18.260000  18.223553   \n",
       "14  2018-01-22  18.260000  18.469999  18.090000  18.469999  18.433134   \n",
       "15  2018-01-23  18.400000  18.459999  18.000000  18.240000  18.203592   \n",
       "16  2018-01-24  18.420000  19.629999  18.420000  19.340000  19.301397   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "      Volume  \n",
       "0   33461800  \n",
       "1   55940900  \n",
       "2   37064900  \n",
       "3   26958200  \n",
       "4   28400000  \n",
       "5   35070900  \n",
       "6   28547700  \n",
       "7   37921500  \n",
       "8   45912100  \n",
       "9   28945400  \n",
       "10  58618300  \n",
       "11  58488900  \n",
       "12  48575800  \n",
       "13  33470200  \n",
       "14  33920000  \n",
       "15  35567700  \n",
       "16  89768200  \n",
       "17         0  \n",
       "18  81989500  \n",
       "19  55726200  \n",
       "20  46203000  \n",
       "21  41576600  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = pd.read_csv('petr4_test.csv')\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.190001],\n",
       "       [16.49    ],\n",
       "       [16.780001],\n",
       "       [16.700001],\n",
       "       [16.74    ],\n",
       "       [17.030001],\n",
       "       [16.92    ],\n",
       "       [16.879999],\n",
       "       [17.040001],\n",
       "       [17.32    ],\n",
       "       [17.35    ],\n",
       "       [17.92    ],\n",
       "       [18.35    ],\n",
       "       [18.309999],\n",
       "       [18.26    ],\n",
       "       [18.4     ],\n",
       "       [18.42    ],\n",
       "       [19.34    ],\n",
       "       [19.620001],\n",
       "       [19.67    ],\n",
       "       [19.77    ],\n",
       "       [19.74    ]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = dataset_test.iloc[:, 1:2].values\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1264 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1   2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2   2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3   2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4   2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "        Volume  \n",
       "0   30182600.0  \n",
       "1   30552600.0  \n",
       "2   36141000.0  \n",
       "3   28069600.0  \n",
       "4   29091300.0  \n",
       "..         ...  \n",
       "17         0.0  \n",
       "18  81989500.0  \n",
       "19  55726200.0  \n",
       "20  46203000.0  \n",
       "21  41576600.0  \n",
       "\n",
       "[1264 rows x 7 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_completed = pd.concat((dataset, dataset_test))\n",
    "dataset_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_completed = dataset_completed.drop('Date', axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 6)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = dataset_completed[len(dataset_completed) - len(dataset_test) - 90:].values\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = normalizer.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 90, 6)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = []\n",
    "\n",
    "for i in range(90, 112):\n",
    "    x_test.append(inputs[i-90:i, 0:6])\n",
    "    \n",
    "x_test = np.array(x_test)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer_prediction = MinMaxScaler(feature_range= (0, 1))\n",
    "normalizer_prediction.fit_transform(dataset.iloc[:, 1:2].values)\n",
    "\n",
    "predictions = normalizer_prediction.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.099188],\n",
       "       [16.229784],\n",
       "       [16.343098],\n",
       "       [16.452255],\n",
       "       [16.560308],\n",
       "       [16.669828],\n",
       "       [16.78189 ],\n",
       "       [16.879951],\n",
       "       [16.962038],\n",
       "       [17.03288 ],\n",
       "       [17.100897],\n",
       "       [17.180065],\n",
       "       [17.305603],\n",
       "       [17.487396],\n",
       "       [17.705324],\n",
       "       [17.927776],\n",
       "       [18.113873],\n",
       "       [18.277636],\n",
       "       [18.451311],\n",
       "       [18.66211 ],\n",
       "       [18.906788],\n",
       "       [19.13954 ]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.190001],\n",
       "       [16.49    ],\n",
       "       [16.780001],\n",
       "       [16.700001],\n",
       "       [16.74    ],\n",
       "       [17.030001],\n",
       "       [16.92    ],\n",
       "       [16.879999],\n",
       "       [17.040001],\n",
       "       [17.32    ],\n",
       "       [17.35    ],\n",
       "       [17.92    ],\n",
       "       [18.35    ],\n",
       "       [18.309999],\n",
       "       [18.26    ],\n",
       "       [18.4     ],\n",
       "       [18.42    ],\n",
       "       [19.34    ],\n",
       "       [19.620001],\n",
       "       [19.67    ],\n",
       "       [19.77    ],\n",
       "       [19.74    ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49865741878440173"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ca3de26290>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test, color= \"red\", label= \"Real Price\")\n",
    "plt.plot(predictions, color= \"blue\", label= \"Predicted Price\")\n",
    "plt.title(\"Prediction of Stock Price\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
